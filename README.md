# TabStruct ğŸ§®  
**Structural Deep Encoding for Table Question Answering (ACL 2025)**

[![ACL 2025](https://img.shields.io/badge/ACL-2025-blue.svg)](https://aclanthology.org/)  
[![Stars](https://img.shields.io/github/stars/RaphaelMouravieff/TabStruct?style=social)](https://github.com/RaphaelMouravieff/TabStruct/stargazers)

ğŸš€ **[Paper](https://arxiv.org/abs/XXXX.YYYY)** | ğŸ“˜ **[Project Page](https://raphaelmouravieff.github.io/TabStruct/)** | ğŸ¥ **[Video Demo](https://www.youtube.com/watch?v=YOUR_VIDEO_ID)**

> TabStruct is a flexible and modular framework for exploring and evaluating structural encodings in Transformer-based table QA. It combines sparse attention, structural embeddings, and token formatting to build robust, scalable models for real and synthetic data.

---

## ğŸ”¥ Key Features

- 128 model configurations combining 5 structural components
- Synthetic and real dataset evaluation 
- Structural generalization, compositionality, and robustness tests
- Fast training with sparse attention (up to **50Ã— speedup** with M3)
- Fully reproducible with one-line setup and job scripts

---


## ğŸ“ Repository Structure
```bash
data/        # Scripts and configs for synthetic table generation
jobs/        # Configurations for different experiments (128)
script/      # Utilities to generate job batches and download data
tabstruct/   # Source code for models, encodings, attention, etc.
run.py       # Main controller script for experiments
```

---

## ğŸ”§ Installation

```bash
git clone https://github.com/RaphaelMouravieff/TabStruct.git TabStruct
cd TabStruct
conda create -n tabstruct python=3.11.11 -y
conda activate tabstruct
pip install -r requirements.txt
```

## ğŸ“¥ Data & Preprocessing

```bash
# Download all necessary datasets (WikiSQL, Synthetic)
bash script/download_data.sh
```

## ğŸ§ª Running Experiments

```bash
# Generate all train/test jobs for the 128 model variants
bash script/generate_all_jobs.sh

# Train on WikiSQL
bash jobs/train/{model_name}/wikisql.sh

# Train on synthetic data
bash jobs/train/{model_name}/synthetic.sh

# Evaluate on compositional generalization
bash jobs/test/{model_name}/compositional.sh

# (Recommended) Run full synthetic tests: compositional, robustness, and structural
bash jobs/test/{model_name}/synthetic.sh
```

## ğŸ§¬ Model Variants

Each model is defined by a unique combination of 5 structural components:
	â€¢	T: Token Structure (T0, T1, T2) - T0 = no special tokens, T2 = row+column+cell markers
	â€¢	E: Structural Embeddings (E0, E1) - E0 = no structure embeddings, E1 = row+column embeddings
	â€¢	PE: Positional Embedding (TPE, CPE) - PE = cell-level, TPE = standard Transformer position encoding
	â€¢	B: Attention Bias (B0, B1) - B0 = no attention bias, B1 = TableFormer-style relational bias
	â€¢	M: Sparse Attention Mask (M0â€“M6) â€” M0 = no sparsity


Example:
T2-M3-TPE-B1-E1

This means:
	â€¢	Tokens: Row+Column+Cell tokens
	â€¢	Mask: Sparse mask M3 (ultra-efficient)
	â€¢	Positional Embedding: Table-wise (TPE)
	â€¢	Bias: Enabled
	â€¢	Structure Embedding: Row+Column Embedding

ğŸ“„ See [all_models.txt](./all_models.txt) for the full list of 128 variants.
ğŸ““ For a visual overview of how each component is applied during model generation, check out:
[Project_Overview.ipynb](./Notebooks/Project_Overview.ipynb)


## ğŸ¥ Demo & GitHub Pages

ğŸ“˜ Project Page
ğŸ¥ Watch the Demo


â¸»

ğŸ“ˆ Results & Benchmarks

TabStruct achieves strong generalization and robustness across synthetic and real datasets.

Model Variant	WikiSQL	Synthetic Avg	Speedup (M3)
T2-M0-TPE-B1-E1	78.5%	79.3%	1Ã—
T2-M3-TPE-B1-E1	80.3%	79.4%	50Ã—
TAPEX (baseline)	74.7%	79.5%	N/A
TableFormer (baseline)	60.5%	23.1%	N/A


â¸»

## ğŸ§ª Reproducing Paper Results

```bash
# Train the best variant on WikiSQL
bash jobs/train/T2-M3-TPE-B1-E1/wikisql.sh

# Evaluate on all generalization tests
bash jobs/test/T2-M3-TPE-B1-E1/synthetic.sh
```

â¸»

ğŸ“Š Datasets & Licenses

This project uses the following datasets:
	â€¢	WikiSQL (Zhong et al., 2017)
â†’ We include a preprocessed version
â†’ License: BSD 3-Clause
â†’ See LICENSE.wikisql for full terms
	â€¢	Synthetic Data
â†’ Fully auto-generated, no human annotation involved
	â€¢	WikiTableQuestions (WTQ)
â†’ Downloaded externally using the provided script

â¸»

ğŸ“œ Citation

If you use TabStruct in your research, please cite us:

@inproceedings{mouravieff2025tabstruct,
  title     = {Structural Deep Encoding for Table Question Answering},
  author    = {Raphael Mouravieff and others},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2025}
}


â¸»

ğŸ“‚ License

This repository is licensed under the MIT License.

Portions of the data are adapted from the WikiSQL dataset by Salesforce.com, Inc.,
which is licensed under the BSD 3-Clause License.

