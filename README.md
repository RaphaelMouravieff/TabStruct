# TabStruct ðŸ§®  
**Structural Deep Encoding for Table Question Answering (ACL 2025)**

[![ACL 2025](https://img.shields.io/badge/ACL-2025-blue.svg)](https://aclanthology.org/)  
[![Stars](https://img.shields.io/github/stars/RaphaelMouravieff/TabStruct?style=social)](https://github.com/RaphaelMouravieff/TabStruct/stargazers)

![TabStruct Overview](./figures/main.png)


ðŸš€ **[Paper](https://arxiv.org/abs/2503.01457es)** | ðŸ“˜ **[Project Page](https://raphaelmouravieff.github.io/Structural-Deep-Encoding-for-Table-Question-Answering/)** | ðŸŽ¥ **[Video Demo](https://www.youtube.com/watch?v=YOUR_VIDEO_ID)**

> TabStruct is a flexible and modular framework for exploring and evaluating structural encodings in Transformer-based table QA. It combines sparse attention, structural embeddings, and token formatting to build robust, scalable models for real and synthetic data.

---

## ðŸ”¥ Key Features

- 128 model configurations combining 5 structural components
- Synthetic and real dataset evaluation 
- Structural generalization, compositionality, and robustness tests
- Fast training with sparse attention (up to **50Ã— speedup** with M3)
- Fully reproducible with one-line setup and job scripts

---


## ðŸ“ Repository Structure
```bash
data/        # Scripts and configs for synthetic table generation
jobs/        # Configurations for different experiments (128)
script/      # Utilities to generate job batches and download data
tabstruct/   # Source code for models, encodings, attention, etc.
run.py       # Main controller script for experiments
```

---

## ðŸ”§ Installation

> ðŸ“Œ **Reproducing results?** Just copy-paste this to get started with TabStruct.

```bash
# Clone the repo
git clone https://github.com/RaphaelMouravieff/TabStruct.git TabStruct
cd TabStruct

# Set up the environment
conda create -n tabstruct python=3.11.11 -y
conda activate tabstruct

# Install dependencies
pip install -r requirements.txt
```

## ðŸ“¥ Data & Preprocessing

ðŸ“¦ Step 1: Download all datasets used in the paper (WikiSQL, Synthetic)
```bash
# Download all necessary datasets (WikiSQL, Synthetic)
bash script/download_data.sh

ðŸ§ª Step 2: Auto-generate all training & evaluation jobs (128 .sh scripts matching our experiments)
After this, you can directly run any script to reproduce results from the paper.
# Generate all train/test jobs for the 128 model variants
bash script/generate_all_jobs.sh
```

## ðŸ§ª Running Experiments

ðŸš‚ Train on WikiSQL (single benchmark)
Test is automatically included in the training script.
```bash
# Train on WikiSQL
bash jobs/train/{model_name}/wikisql.sh
```

ðŸ§¬ Train on Synthetic Data (multi-benchmark setup)
```bash
# Train on synthetic data
bash jobs/train/{model_name}/synthetic.sh
```
ðŸ§ª After training, run one or more generalization tests:
```bash
# Evaluate on compositional generalization
bash jobs/test/{model_name}/compositional.sh

# (Recommended) Run full synthetic tests: compositional, robustness, and structural
bash jobs/test/{model_name}/synthetic.sh
```
ðŸ“„ See all valid {model_name} variants in: [all_models.txt](./all_models.txt)

## ðŸ§¬ Model Variants

Each model is defined by a unique combination of 5 structural components:

- **T: Token Structure (T0, T1, T2)** â€“ T0 = no special tokens, T2 = row+column+cell markers  
- **E: Structural Embeddings (E0, E1)** â€“ E0 = no structure embeddings, E1 = row+column embeddings  
- **PE: Positional Embedding (TPE, CPE)** â€“ PE = cell-level, TPE = standard Transformer position encoding  
- **B: Attention Bias (B0, B1)** â€“ B0 = no attention bias, B1 = TableFormer-style relational bias  
- **M: Sparse Attention Mask (M0â€“M6)** â€“ M0 = no sparsity


Example:
T2-M3-TPE-B1-E1

This means:

- **Tokens**: Row+Column+Cell tokens  
- **Mask**: Sparse mask M3 (ultra-efficient)  
- **Positional Embedding**: Table-wise (TPE)  
- **Bias**: Enabled  
- **Structure Embedding**: Row+Column Embedding

ðŸ“„ See [all_models.txt](./all_models.txt) for the full list of 128 variants.
ðŸ““ For a visual overview of how each component is applied during model generation, check out:
[Project_Overview.ipynb](./Notebooks/Project_Overview.ipynb)


## ðŸŽ¥ Demo & GitHub Pages

ðŸ“˜ [Project Page](https://raphaelmouravieff.github.io/Structural-Deep-Encoding-for-Table-Question-Answering/)

ðŸŽ¥ [Video Demo](https://www.youtube.com/watch?v=YOUR_VIDEO_ID)


â¸»

## ðŸ§ª Reproducing Paper Results

```bash
# Train the best variant on WikiSQL
bash jobs/train/T2-M3-TPE-B1-E1/wikisql.sh

# Evaluate on all generalization tests
bash jobs/test/T2-M3-TPE-B1-E1/synthetic.sh
```

â¸»

## ðŸ“œ **Citation**

If you use TabStruct in your research, please cite us:

```bibtex
@article{mouravieff2025structural,
  title={Structural Deep Encoding for Table Question Answering},
  author={Mouravieff, Rapha{\"e}l and Piwowarski, Benjamin and Lamprier, Sylvain},
  journal={arXiv preprint arXiv:2503.01457},
  year={2025}
}
```

â¸»

## ðŸ“‚ License

This repository is licensed under the MIT License.

Portions of the data are adapted from the WikiSQL dataset by Salesforce.com, Inc.,
which is licensed under the BSD 3-Clause License.



This project uses the following datasets:

- **WikiSQL** (Zhong et al., 2017)  
  â†’ We include a preprocessed version  
  â†’ License: BSD 3-Clause  
  â†’ See `LICENSE.wikisql` for full terms

- **Synthetic Data**  
  â†’ Fully auto-generated